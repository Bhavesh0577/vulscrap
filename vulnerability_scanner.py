import requests
import logging
import sqlite3
import json
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util import Retry
from playwright.sync_api import sync_playwright
try:
    from playwright_stealth import stealth
    stealth_sync = stealth  # Alias for compatibility
except ImportError:
    stealth_sync = None  # Fallback if not available
from requests.auth import HTTPBasicAuth

try:
    from openvas_integration import (
        OpenVASConfig,
        OpenVASScanResult,
        OPENVAS_DEPENDENCIES_AVAILABLE,
        OpenVASConfigurationError,
        docker_backend_available,
        load_openvas_config,
        run_openvas_scan,
    )
except ImportError:  # pragma: no cover - optional dependency
    OpenVASConfig = Any  # type: ignore
    OpenVASScanResult = Any  # type: ignore
    OPENVAS_DEPENDENCIES_AVAILABLE = False

    def docker_backend_available() -> bool:  # type: ignore
        return False

    class OpenVASConfigurationError(RuntimeError):
        """Fallback error when OpenVAS integration is unavailable."""

    def load_openvas_config(*_args, **_kwargs):  # type: ignore
        raise OpenVASConfigurationError(
            "OpenVAS integration module not available in the current environment"
        )

    def run_openvas_scan(*_args, **_kwargs):  # type: ignore
        raise OpenVASConfigurationError(
            "OpenVAS integration module not available in the current environment"
        )

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("vulnerability_scraper.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Define database path
DB_PATH = 'vulnerabilities.db'

# Define OEM sources with their respective URLs and selectors
OEM_SOURCES = {
    "Cisco": {
        "url": "https://sec.cloudapps.cisco.com/security/center/publicationListing.x",
        "selector": "div.advisory-list-item, tr.advisory-list-item, table.table-striped tr",
        "requires_js": True,
        "mapping": {
            "product_name": {"selector": "a.advisory-id", "attribute": "text"},
            "severity": {"selector": "span.impact-label, td.impact", "attribute": "text"},
            "description": {"selector": "div.summary", "attribute": "text"},
            "cve_id": {"selector": "span.cve-id, td.cve", "attribute": "text"},
            "pub_date": {"selector": "span.last-updated, td.last-updated", "attribute": "text"},
            "url": {"selector": "a.advisory-id", "attribute": "href", "is_url": True}
        }
    },
    "Microsoft": {
        "url": "https://msrc.microsoft.com/update-guide/en-us/vulnerability",
        "selector": "div.vuln-list div.vuln-item",
        "requires_js": True,
        "mapping": {
            "product_name": {"selector": "div.product-name", "attribute": "text"},
            "severity": {"selector": "div.severity", "attribute": "text"},
            "description": {"selector": "div.description", "attribute": "text"},
            "cve_id": {"selector": "div.cve-id", "attribute": "text"},
            "pub_date": {"selector": "div.pub-date", "attribute": "text"},
            "url": {"selector": "a.vuln-detail-link", "attribute": "href", "is_url": True}
        }
    },
    "Google": {
        "url": "https://cloud.google.com/support/bulletins",
        "selector": "table tbody tr",
        "requires_js": True,
        "mapping": {
            "advisory_id": {"selector": "td:nth-child(1) a", "attribute": "text"},
            "description": {"selector": "td:nth-child(1) a", "attribute": "text"},
            "severity": {"selector": "td:nth-child(3)", "attribute": "text"},
            "cve_id": {"selector": "td:nth-child(4) a", "attribute": "text"},
            "pub_date": {"selector": "td:nth-child(2)", "attribute": "text"},
            "url": {"selector": "td:nth-child(1) a", "attribute": "href", "is_url": True}
        }
    },
    "Oracle": {
        "url": "https://www.oracle.com/security-alerts/",
        "selector": "table.DataTable tbody tr",
        "requires_js": False,
        "mapping": {
            "product_name": {"selector": "td:nth-child(1)", "attribute": "text"},
            "description": {"selector": "td:nth-child(2)", "attribute": "text"},
            "cve_id": {"selector": "td:nth-child(3)", "attribute": "text"},
            "pub_date": {"selector": "td:nth-child(4)", "attribute": "text"},
            "severity": {"selector": "td:nth-child(5)", "attribute": "text"},
            "url": {"selector": "td:nth-child(2) a", "attribute": "href", "is_url": True}
        }
    },
    "VMware": {
        "url": "https://www.vmware.com/security/advisories.html",
        "selector": "div.security-advisory",
        "requires_js": True,
        "mapping": {
            "product_name": {"selector": "div.product-affected", "attribute": "text"},
            "cve_id": {"selector": "div.advisory-id", "attribute": "text"},
            "description": {"selector": "div.advisory-description", "attribute": "text"},
            "severity": {"selector": "div.advisory-severity", "attribute": "text"},
            "pub_date": {"selector": "div.advisory-date", "attribute": "text"},
            "url": {"selector": "a.advisory-link", "attribute": "href", "is_url": True}
        }
    },
    "IBM": {
        "url": "https://www.ibm.com/support/pages/security-bulletins",
        "selector": "table.ibm-data-table tbody tr",
        "requires_js": True,
        "mapping": {
            "product_name": {"selector": "td:nth-child(1)", "attribute": "text"},
            "description": {"selector": "td:nth-child(2)", "attribute": "text"},
            "cve_id": {"selector": "td:nth-child(3)", "attribute": "text"},
            "severity": {"selector": "td:nth-child(4)", "attribute": "text"},
            "pub_date": {"selector": "td:nth-child(5)", "attribute": "text"},
            "url": {"selector": "td:nth-child(2) a", "attribute": "href", "is_url": True}
        }
    },
    "Adobe": {
        "url": "https://helpx.adobe.com/in/security.html",
        "selector": "div.security-bulletin",
        "requires_js": True,
        "mapping": {
            "product_name": {"selector": "h3.bulletin-title", "attribute": "text"},
            "description": {"selector": "div.bulletin-summary", "attribute": "text"},
            "severity": {"selector": "span.severity-rating", "attribute": "text"},
            "pub_date": {"selector": "span.bulletin-date", "attribute": "text"},
            "cve_id": {"selector": "span.cve-id", "attribute": "text"},
            "url": {"selector": "a.bulletin-link", "attribute": "href", "is_url": True}
        }
    },
    "HPE": {
        "url": "https://support.hpe.com/hpesc/public/home/productBulletinBoard",
        "selector": "div.security-bulletin-item",
        "requires_js": True,
        "mapping": {
            "product_name": {"selector": "span.product-name", "attribute": "text"},
            "description": {"selector": "div.bulletin-summary", "attribute": "text"},
            "cve_id": {"selector": "span.cve-reference", "attribute": "text"},
            "severity": {"selector": "span.severity-indicator", "attribute": "text"},
            "pub_date": {"selector": "span.publication-date", "attribute": "text"},
            "url": {"selector": "a.bulletin-details-link", "attribute": "href", "is_url": True}
        }
    },
    "NVD": {
        "url": "https://services.nvd.nist.gov/rest/json/cves/2.0?resultsPerPage=20",
        "is_api": True,
        "mapping": {
            "cve_id": "id",
            "description": "descriptions[0].value",
            "severity": "metrics.cvssMetricV31[0].cvssData.baseSeverity",
            "pub_date": "published",
            "product_name": "configurations.nodes[0].cpeMatch[0].criteria",
            "url": "id"
        }
    },
    "CISA": {
        "url": "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json",
        "is_api": True,
        "mapping": {
            "cve_id": "cveID",
            "product_name": "product",
            "vendor": "vendorProject",
            "description": "shortDescription",
            "severity": "Critical",  # All CISA KEV vulnerabilities are considered critical
            "pub_date": "dateAdded",
            "mitigation": "requiredAction",
            "url": "notes"
        }
    },
    "Microsoft Security": {
        "url": "https://api.msrc.microsoft.com/cvrf/v2.0/updates",
        "type": "api"
    },
    "Cisco Security": {
        "url": "https://tools.cisco.com/security/center/publicationListing.x",
        "type": "web"
    }
}

def setup_database():
    """Create SQLite database and tables if they don't exist"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # Create vulnerabilities table
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS vulnerabilities (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        product_name TEXT,
        product_version TEXT,
        oem_name TEXT,
        severity_level TEXT,
        vulnerability_description TEXT,
        mitigation_strategy TEXT,
        published_date TEXT,
        cve_id TEXT UNIQUE,
        url TEXT,
        discovered_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        notified BOOLEAN DEFAULT FALSE
    )
    ''')
    
    # Create recipients table
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS recipients (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        email TEXT UNIQUE
    )
    ''')
    
    conn.commit()
    conn.close()
    logger.info("Database setup completed")

def create_requests_session():
    """Create a requests session with retry mechanism"""
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=0.5,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

def scrape_with_playwright(url, source_config):
    """Scrape content using Playwright for JavaScript-heavy sites"""
    vulnerabilities = []
    
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()
        
        # Apply stealth mode to avoid detection (if available)
        if stealth_sync:
            try:
                stealth_sync(page)
            except Exception as e:
                logger.warning(f"Stealth mode not available: {str(e)}")
        
        try:
            logger.info(f"Navigating to {url}")
            page.goto(url, wait_until="networkidle")
            
            # Wait for the content to load
            page.wait_for_selector(source_config["selector"])
            
            # Get the page content
            content = page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # Extract vulnerability information
            elements = soup.select(source_config["selector"])
            
            for element in elements:
                vulnerability = extract_vulnerability_data(element, source_config["mapping"], url)
                if vulnerability:
                    vulnerabilities.append(vulnerability)
            
        except Exception as e:
            logger.error(f"Error scraping {url} with Playwright: {str(e)}")
        
        finally:
            browser.close()
    
    return vulnerabilities

def scrape_with_requests(url, source_config):
    """Scrape content using requests for static sites"""
    vulnerabilities = []
    session = create_requests_session()
    
    try:
        logger.info(f"Requesting {url}")
        response = session.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Extract vulnerability information
        elements = soup.select(source_config["selector"])
        
        for element in elements:
            vulnerability = extract_vulnerability_data(element, source_config["mapping"], url)
            if vulnerability:
                vulnerabilities.append(vulnerability)
                
    except Exception as e:
        logger.error(f"Error scraping {url} with requests: {str(e)}")
    
    return vulnerabilities

def fetch_nvd_vulnerabilities(url):
    """Fetch vulnerabilities from NVD API"""
    vulnerabilities = []
    session = create_requests_session()
    
    try:
        logger.info(f"Requesting NVD API: {url}")
        response = session.get(url, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        
        if 'vulnerabilities' in data:
            for vuln_data in data['vulnerabilities']:
                vuln = vuln_data.get('cve', {})
                
                vulnerability = {}
                vulnerability["cve_id"] = vuln.get('id', 'N/A')
                
                # Extract description
                descriptions = vuln.get('descriptions', [])
                for desc in descriptions:
                    if desc.get('lang') == 'en':
                        vulnerability["vulnerability_description"] = desc.get('value', 'N/A')
                        break
                
                # Extract severity
                metrics = vuln.get('metrics', {})
                cvss_v3 = metrics.get('cvssMetricV31', [{}])[0] if 'cvssMetricV31' in metrics else metrics.get('cvssMetricV30', [{}])[0] if 'cvssMetricV30' in metrics else {}
                cvss_data = cvss_v3.get('cvssData', {})
                vulnerability["severity_level"] = standardize_severity(cvss_data.get('baseSeverity', 'N/A'))
                
                # Extract published date
                vulnerability["published_date"] = standardize_date(vuln.get('published', 'N/A'))
                
                # Extract product information
                configurations = vuln.get('configurations', {})
                nodes = configurations.get('nodes', [])
                products = []
                
                for node in nodes:
                    cpe_matches = node.get('cpeMatch', [])
                    for cpe in cpe_matches:
                        if cpe.get('criteria'):
                            # Extract product name from CPE string
                            cpe_parts = cpe.get('criteria').split(':')
                            if len(cpe_parts) > 4:
                                products.append(cpe_parts[4])
                
                vulnerability["product_name"] = ', '.join(products) if products else 'N/A'
                
                # Set URL and other fields
                vulnerability["url"] = f"https://nvd.nist.gov/vuln/detail/{vulnerability['cve_id']}"
                vulnerability["mitigation_strategy"] = f"See {vulnerability['url']} for mitigation details"
                vulnerability["oem_name"] = "NVD"
                vulnerability["product_version"] = "N/A"
                
                vulnerabilities.append(vulnerability)
        
    except Exception as e:
        logger.error(f"Error fetching NVD vulnerabilities: {str(e)}")
    
    return vulnerabilities

def extract_vulnerability_data(element, mapping, base_url):
    """Extract vulnerability data based on the mapping configuration"""
    vulnerability = {}
    
    try:
        for field, config in mapping.items():
            selector = config["selector"]
            attribute = config["attribute"]
            
            selected_element = element.select_one(selector)
            if selected_element:
                if attribute == "text":
                    value = selected_element.text.strip()
                else:
                    value = selected_element.get(attribute, "")
                
                # Handle URL fields
                if config.get("is_url", False) and value:
                    if not value.startswith(("http://", "https://")):
                        value = base_url + value if not value.startswith("/") else base_url + "/" + value
                
                # Map to our standardized fields
                if field == "product_name":
                    vulnerability["product_name"] = value
                elif field == "advisory_id":
                    # For Google Cloud bulletins, use advisory_id as product_name
                    vulnerability["product_name"] = f"Google Cloud - {value}"
                    # Also save advisory_id for possible CVE generation
                    vulnerability["advisory_id"] = value
                elif field == "severity":
                    vulnerability["severity_level"] = standardize_severity(value)
                elif field == "description":
                    vulnerability["vulnerability_description"] = value
                elif field == "cve_id":
                    vulnerability["cve_id"] = value
                elif field == "pub_date":
                    vulnerability["published_date"] = standardize_date(value)
                elif field == "url":
                    vulnerability["url"] = value
                    # Try to extract mitigation from URL
                    vulnerability["mitigation_strategy"] = f"See {value} for mitigation details"
        
        # Extract OEM name from the base URL
        domain = base_url.split("//")[1].split(".")[0]
        vulnerability["oem_name"] = domain.capitalize()
        
        # Default product version if not available
        vulnerability["product_version"] = "N/A"
        
        # For Google bulletins, generate a unique CVE ID if none is provided
        if vulnerability.get("oem_name") == "Google" and not vulnerability.get("cve_id"):
            advisory_id = vulnerability.get("advisory_id", "GCP-NONE")
            vulnerability["cve_id"] = f"GCP-{advisory_id}" if advisory_id.startswith("GCP-") else advisory_id 
        
    except Exception as e:
        logger.error(f"Error extracting vulnerability data: {str(e)}")
        return None
    
    return vulnerability

def standardize_severity(severity):
    """Standardize severity levels across different sources.
    
    Always returns one of: Critical, High, Medium, Low.
    Any unrecognised input (CVE IDs, template tags, etc.) maps to Medium.
    """
    if not severity or not isinstance(severity, str):
        return "Medium"
    severity = severity.strip().lower()
    
    if any(term in severity for term in ["critical"]):
        return "Critical"
    elif any(term in severity for term in ["high", "important"]):
        return "High"
    elif any(term in severity for term in ["medium", "moderate"]):
        return "Medium"
    elif any(term in severity for term in ["low", "informational", "info"]):
        return "Low"
    else:
        # Anything else (CVE IDs, template tags like {{list.cve}}, numbers, etc.)
        return "Medium"

def standardize_date(date_str):
    """Standardize date formats across different sources"""
    try:
        # Handle ISO format from NVD API
        if 'T' in date_str:
            try:
                parsed_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                return parsed_date.strftime("%b %Y")
            except ValueError:
                pass
        
        # Add more date format handling as needed
        formats = [
            "%Y-%m-%d", 
            "%d %b %Y", 
            "%b %d, %Y",
            "%d/%m/%Y",
            "%m/%d/%Y"
        ]
        
        for fmt in formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                return parsed_date.strftime("%b %Y")
            except ValueError:
                continue
        
        # If no format matches, return the original
        return date_str
    except Exception:
        return date_str

def save_vulnerability_to_db(vulnerability):
    """Save vulnerability to database if it doesn't exist"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        # Check if vulnerability already exists
        cursor.execute("SELECT id FROM vulnerabilities WHERE cve_id = ?", (vulnerability.get("cve_id"),))
        result = cursor.fetchone()
        
        if not result:
            # Insert new vulnerability
            cursor.execute('''
            INSERT INTO vulnerabilities (
                product_name, 
                product_version, 
                oem_name, 
                severity_level, 
                vulnerability_description, 
                mitigation_strategy, 
                published_date, 
                cve_id, 
                url,
                notified
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                vulnerability.get("product_name", "N/A"),
                vulnerability.get("product_version", "N/A"),
                vulnerability.get("oem_name", "N/A"),
                vulnerability.get("severity_level", "N/A"),
                vulnerability.get("vulnerability_description", "N/A"),
                vulnerability.get("mitigation_strategy", "N/A"),
                vulnerability.get("published_date", "N/A"),
                vulnerability.get("cve_id", "N/A"),
                vulnerability.get("url", "N/A"),
                False
            ))
            conn.commit()
            return True
        
        return False
    
    except sqlite3.IntegrityError as e:
        # Handle unique constraint violation silently
        conn.rollback()
        return False
    except Exception as e:
        logger.error(f"Error saving vulnerability to database: {str(e)}")
        conn.rollback()
        return False
    
    finally:
        conn.close()

def get_vulnerabilities(severity_filter=None):
    """Get vulnerabilities from database with optional severity filter"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        if severity_filter:
            if isinstance(severity_filter, list):
                placeholders = ", ".join(["?" for _ in severity_filter])
                cursor.execute(f'''
                    SELECT product_name, product_version, oem_name, severity_level,
                           vulnerability_description, mitigation_strategy, published_date,
                           cve_id, url
                    FROM vulnerabilities
                    WHERE severity_level IN ({placeholders})
                    ORDER BY published_date DESC
                ''', severity_filter)
            else:
                cursor.execute('''
                    SELECT product_name, product_version, oem_name, severity_level,
                           vulnerability_description, mitigation_strategy, published_date,
                           cve_id, url
                    FROM vulnerabilities
                    WHERE severity_level = ?
                    ORDER BY published_date DESC
                ''', (severity_filter,))
        else:
            cursor.execute('''
                SELECT product_name, product_version, oem_name, severity_level,
                       vulnerability_description, mitigation_strategy, published_date,
                       cve_id, url
                FROM vulnerabilities
                ORDER BY published_date DESC
            ''')
        
        columns = ['product_name', 'product_version', 'oem_name', 'severity_level',
                  'vulnerability_description', 'mitigation_strategy', 'published_date',
                  'cve_id', 'url']
        
        vulnerabilities = []
        for row in cursor.fetchall():
            vulnerability = dict(zip(columns, row))
            vulnerabilities.append(vulnerability)
        
        return vulnerabilities
    
    except Exception as e:
        logging.error(f"Error getting vulnerabilities: {str(e)}")
        return []
    
    finally:
        conn.close()

def add_recipient(email):
    """Add email recipient to database"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        cursor.execute("INSERT OR IGNORE INTO recipients (email) VALUES (?)", (email,))
        conn.commit()
        logger.info(f"Added recipient: {email}")
        return True
    
    except Exception as e:
        logger.error(f"Error adding recipient: {str(e)}")
        conn.rollback()
        return False
    
    finally:
        conn.close()

def get_recipients():
    """Get email recipients from database"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        cursor.execute("SELECT email FROM recipients")
        return [row[0] for row in cursor.fetchall()]
    
    except Exception as e:
        logger.error(f"Error getting recipients: {str(e)}")
        return []
    
    finally:
        conn.close()

def delete_recipient(email):
    """Delete email recipient from database"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    try:
        cursor.execute("DELETE FROM recipients WHERE email = ?", (email,))
        conn.commit()
        logger.info(f"Deleted recipient: {email}")
        return True
    
    except Exception as e:
        logger.error(f"Error deleting recipient: {str(e)}")
        conn.rollback()
        return False
    
    finally:
        conn.close()

def scan_nvd():
    """Scan NVD API for vulnerabilities"""
    try:
        # NVD API endpoint with parameters
        base_url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
        params = {
            "resultsPerPage": 20,
            "startIndex": 0,
            "noRejected": True,
            "sortBy": "publishDate",
            "sortOrder": "desc"
        }
        
        headers = {
            "User-Agent": "VulnerabilityScannerTool/1.0"
        }
        
        logging.info("Fetching vulnerabilities from NVD...")
        response = requests.get(base_url, params=params, headers=headers)
        response.raise_for_status()
        data = response.json()
        
        vulnerabilities = []
        if 'vulnerabilities' in data:
            for vuln_data in data['vulnerabilities']:
                cve = vuln_data.get('cve', {})
                
                # Get metrics if available
                metrics = cve.get('metrics', {})
                cvss_v31 = metrics.get('cvssMetricV31', [{}])[0].get('cvssData', {}) if metrics.get('cvssMetricV31') else {}
                cvss_v30 = metrics.get('cvssMetricV30', [{}])[0].get('cvssData', {}) if metrics.get('cvssMetricV30') else {}
                
                # Use CVSS 3.1 if available, fall back to 3.0
                base_score = cvss_v31.get('baseScore', cvss_v30.get('baseScore', 0))
                
                # Determine severity based on base score
                severity = 'Low'
                if base_score >= 9.0:
                    severity = 'Critical'
                elif base_score >= 7.0:
                    severity = 'High'
                elif base_score >= 4.0:
                    severity = 'Medium'
                
                # Get affected products
                affected = cve.get('configurations', [])
                products = set()
                versions = set()
                
                for config in affected:
                    for node in config.get('nodes', []):
                        for cpe in node.get('cpeMatch', []):
                            cpe_parts = cpe.get('criteria', '').split(':')
                            if len(cpe_parts) > 4:
                                products.add(cpe_parts[4])
                                if len(cpe_parts) > 5:
                                    versions.add(cpe_parts[5])
                
                product_name = ', '.join(products) if products else 'Unknown'
                product_version = ', '.join(versions) if versions else 'Multiple'
                
                # Get description (English only)
                description = 'No description available'
                for desc in cve.get('descriptions', []):
                    if desc.get('lang') == 'en':
                        description = desc.get('value', 'No description available')
                        break
                
                # Create vulnerability entry
                vulnerability = {
                    'product_name': product_name,
                    'product_version': product_version,
                    'oem_name': 'NVD',
                    'severity_level': severity,
                    'vulnerability_description': description,
                    'mitigation_strategy': 'See reference for mitigation details',
                    'published_date': cve.get('published', '').split('T')[0] if cve.get('published') else '',
                    'cve_id': cve.get('id', ''),
                    'url': f"https://nvd.nist.gov/vuln/detail/{cve.get('id', '')}"
                }
                
                # Only include if it has a severity and CVE ID
                if vulnerability['severity_level'] and vulnerability['cve_id']:
                    vulnerabilities.append(vulnerability)
                    # Save to database immediately
                    save_vulnerability_to_db(vulnerability)
        
        logging.info(f"Found {len(vulnerabilities)} vulnerabilities from NVD")
        return vulnerabilities
        
    except requests.RequestException as e:
        logging.error(f"Error fetching from NVD API: {str(e)}")
        return []
    except Exception as e:
        logging.error(f"Error processing NVD data: {str(e)}")
        return []

def scan_source(source_name):
    """Scan a specific source for vulnerabilities"""
    logging.info(f"Starting scan for source: {source_name}")
    
    _dispatch = {
        "NVD": scan_nvd,
        "CISA": scan_cisa,
        "Cisco": scan_cisco,
        "Google": scan_google,
        "Microsoft": scan_microsoft,
        "Fortinet": scan_fortinet,
        "Palo Alto": scan_palo_alto,
        "Adobe": scan_adobe,
    }
    handler = _dispatch.get(source_name)
    if handler:
        return handler()
    logging.warning(f"Source {source_name} is not currently supported")
    return []

def scan_cisa():
    """Scan CISA Known Exploited Vulnerabilities catalog — recent entries only."""
    try:
        url = "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
        
        logging.info("Fetching vulnerabilities from CISA KEV catalog...")
        headers = {"User-Agent": "VulnGuard/2.0"}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        vulnerabilities = []
        # Only keep entries added in the last 90 days to avoid flooding
        from datetime import timedelta
        cutoff = datetime.now() - timedelta(days=90)

        for vuln in data.get('vulnerabilities', []):
            date_added = vuln.get('dateAdded', '')
            try:
                added_dt = datetime.strptime(date_added, '%Y-%m-%d')
                if added_dt < cutoff:
                    continue
            except (ValueError, TypeError):
                pass  # if date parsing fails, include it

            vulnerability = {
                'product_name': vuln.get('product', 'N/A'),
                'product_version': 'Multiple',
                'oem_name': vuln.get('vendorProject', 'CISA'),
                'severity_level': 'Critical',
                'vulnerability_description': vuln.get('shortDescription', 'N/A'),
                'mitigation_strategy': vuln.get('requiredAction', 'N/A'),
                'published_date': date_added,
                'cve_id': vuln.get('cveID', 'N/A'),
                'url': f"https://nvd.nist.gov/vuln/detail/{vuln.get('cveID', '')}"
            }
            
            save_vulnerability_to_db(vulnerability)
            vulnerabilities.append(vulnerability)
        
        logging.info(f"Found {len(vulnerabilities)} recent CISA KEV vulnerabilities")
        return vulnerabilities
        
    except requests.RequestException as e:
        logging.error(f"Error fetching from CISA API: {str(e)}")
        return []
    except Exception as e:
        logging.error(f"Error processing CISA data: {str(e)}")
        return []

def scan_cisco():
    """Scan Cisco Security Advisories via the public openvuln API or RSS/web fallback."""
    try:
        logger.info("Scanning Cisco Security Advisories...")
        vulnerabilities = []
        
        # --- Strategy 1: Cisco Security Advisories public page (no auth needed) ---
        # The Cisco PSIRT openVuln API requires registered credentials.
        # We'll scrape the public security advisories listing instead.
        
        session = create_requests_session()
        headers = {
            "User-Agent": "VulnGuard/2.0 (Security Vulnerability Scanner)",
            "Accept": "application/json, text/html",
        }
        
        # Try the Cisco Security Advisories JSON feed first
        json_feed_url = "https://sec.cloudapps.cisco.com/security/center/publicationService.x?criteria=exact&cves=&keyword=&last_published_date=last_month&limit=40&offset=0&publicationTypeIDs=1,3&securityImpactRatings=critical,high&sort=-day_sir"
        
        try:
            logger.info("Trying Cisco security publications JSON feed...")
            resp = session.get(json_feed_url, headers=headers, timeout=30)
            
            if resp.status_code == 200:
                try:
                    data = resp.json()
                    # Handle both list and dict responses
                    advisories = data if isinstance(data, list) else data.get("advisories", data.get("publications", []))
                    
                    for adv in advisories:
                        cve_str = adv.get("cves", adv.get("cve", ""))
                        # May be a list or comma-sep string
                        if isinstance(cve_str, list):
                            cve_id = cve_str[0] if cve_str else ""
                        else:
                            cve_id = str(cve_str).split(",")[0].strip() if cve_str else ""
                        
                        adv_id = adv.get("advisoryId", adv.get("identifier", ""))
                        if not cve_id:
                            cve_id = f"CISCO-{adv_id}" if adv_id else f"CISCO-{len(vulnerabilities)}"
                        
                        title = adv.get("advisoryTitle", adv.get("title", ""))
                        severity = adv.get("sir", adv.get("severity", adv.get("securityImpactRating", "")))
                        summary = adv.get("summary", adv.get("headline", title))
                        pub_url = adv.get("publicationUrl", adv.get("url", ""))
                        pub_date = adv.get("lastUpdated", adv.get("firstPublished", adv.get("lastPublished", "")))
                        
                        products = adv.get("productNames", [])
                        product_str = ", ".join(products[:5]) if isinstance(products, list) and products else title.split(" Vulnerability")[0] if title else "Cisco Product"

                        vulnerability = {
                            "product_name": product_str,
                            "product_version": "Multiple",
                            "oem_name": "Cisco",
                            "severity_level": standardize_severity(severity) if severity else "High",
                            "vulnerability_description": summary[:500] if summary else "See advisory",
                            "mitigation_strategy": f"See {pub_url}" if pub_url else "See Cisco advisory",
                            "published_date": standardize_date(pub_date) if pub_date else "",
                            "cve_id": cve_id,
                            "url": pub_url or f"https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/{adv_id}",
                        }
                        save_vulnerability_to_db(vulnerability)
                        vulnerabilities.append(vulnerability)
                    
                    if vulnerabilities:
                        logger.info(f"Found {len(vulnerabilities)} Cisco advisories via JSON feed")
                        return vulnerabilities
                except (ValueError, KeyError) as e:
                    logger.warning(f"Could not parse Cisco JSON feed: {e}")
        except requests.RequestException as e:
            logger.warning(f"Cisco JSON feed unavailable: {e}")
        
        # --- Strategy 2: Scrape the HTML advisories page ---
        logger.info("Falling back to Cisco HTML advisories page...")
        html_url = "https://tools.cisco.com/security/center/publicationListing.x"
        try:
            resp = session.get(html_url, headers=headers, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "html.parser")
            
            for row in soup.select("tr.ng-scope, div.advisory-list-item, tr"):
                try:
                    link = row.select_one("a[href*='CiscoSecurityAdvisory']") or row.select_one("a")
                    if not link:
                        continue
                    title_text = link.get_text(strip=True)
                    href = link.get("href", "")
                    if href and not href.startswith("http"):
                        href = "https://sec.cloudapps.cisco.com" + href
                    
                    sev_el = row.select_one("td.impact, span.impact-label, td:nth-child(3)")
                    severity_text = sev_el.get_text(strip=True) if sev_el else "High"
                    
                    date_el = row.select_one("td.last-updated, span.last-updated, td:nth-child(4)")
                    date_text = date_el.get_text(strip=True) if date_el else ""
                    
                    cve_el = row.select_one("td.cve, span.cve-id")
                    cve_text = cve_el.get_text(strip=True) if cve_el else f"CISCO-{hash(title_text) % 100000}"
                    
                    vulnerability = {
                        "product_name": title_text[:200],
                        "product_version": "Multiple",
                        "oem_name": "Cisco",
                        "severity_level": standardize_severity(severity_text),
                        "vulnerability_description": title_text,
                        "mitigation_strategy": f"See {href}" if href else "See Cisco advisory",
                        "published_date": standardize_date(date_text) if date_text else "",
                        "cve_id": cve_text,
                        "url": href,
                    }
                    save_vulnerability_to_db(vulnerability)
                    vulnerabilities.append(vulnerability)
                except Exception:
                    continue
            
            if vulnerabilities:
                logger.info(f"Found {len(vulnerabilities)} Cisco advisories via HTML scrape")
                return vulnerabilities
        except Exception as e:
            logger.warning(f"Cisco HTML scrape failed: {e}")
        
        # --- Strategy 3: Cisco IOS-XE RSS as last resort ---
        logger.info("Trying Cisco RSS feed...")
        rss_url = "https://tools.cisco.com/security/center/psirtrss20/CiscoSecurityAdvisory.xml"
        try:
            resp = session.get(rss_url, headers=headers, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "xml")
            
            for item in soup.find_all("item"):
                title = item.find("title").text.strip() if item.find("title") else ""
                link = item.find("link").text.strip() if item.find("link") else ""
                desc = item.find("description").text.strip() if item.find("description") else title
                pub_date = item.find("pubDate").text.strip() if item.find("pubDate") else ""
                
                # Try to extract CVE from description
                import re
                cve_match = re.search(r'CVE-\d{4}-\d+', desc)
                cve_id = cve_match.group(0) if cve_match else f"CISCO-{hash(title) % 100000}"
                
                vulnerability = {
                    "product_name": title[:200],
                    "product_version": "Multiple",
                    "oem_name": "Cisco",
                    "severity_level": "High",
                    "vulnerability_description": desc[:500],
                    "mitigation_strategy": f"See {link}",
                    "published_date": standardize_date(pub_date),
                    "cve_id": cve_id,
                    "url": link,
                }
                save_vulnerability_to_db(vulnerability)
                vulnerabilities.append(vulnerability)
            
            logger.info(f"Found {len(vulnerabilities)} Cisco advisories via RSS")
        except Exception as e:
            logger.warning(f"Cisco RSS feed failed: {e}")
        
        if not vulnerabilities:
            logger.warning("All Cisco scan strategies failed — returning empty list")
        return vulnerabilities
        
    except Exception as e:
        logger.error(f"Error in Cisco scanning: {str(e)}")
        return []

def scan_google():
    """Scan Google Cloud security bulletins with multiple strategies."""
    try:
        logger.info("Scanning Google Cloud Security Bulletins...")
        vulnerabilities = []
        session = create_requests_session()
        headers = {"User-Agent": "VulnGuard/2.0 (Security Scanner)", "Accept": "text/html"}

        # --- Strategy 1: Google Cloud Security Bulletins page ---
        bulletin_url = "https://cloud.google.com/support/bulletins"
        try:
            resp = session.get(bulletin_url, headers=headers, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "html.parser")

            # Try tables first
            table = soup.find("table")
            if table:
                rows = table.find_all("tr")[1:]  # skip header
                for row in rows:
                    cells = row.find_all("td")
                    if len(cells) < 3:
                        continue
                    try:
                        link_el = cells[0].find("a")
                        adv_id = link_el.get_text(strip=True) if link_el else cells[0].get_text(strip=True)
                        adv_url = link_el.get("href", "") if link_el else ""
                        if adv_url and not adv_url.startswith("http"):
                            adv_url = "https://cloud.google.com" + adv_url

                        date_text = cells[1].get_text(strip=True) if len(cells) > 1 else ""
                        sev_text = cells[2].get_text(strip=True) if len(cells) > 2 else "Medium"

                        # CVE may be in the 4th column or embedded in text
                        cve_id = ""
                        if len(cells) > 3:
                            cve_el = cells[3].find("a") or cells[3]
                            cve_id = cve_el.get_text(strip=True)
                        if not cve_id:
                            import re as _re
                            m = _re.search(r"CVE-\d{4}-\d+", str(row))
                            cve_id = m.group(0) if m else f"GCP-{adv_id}"

                        vulnerability = {
                            "product_name": f"Google Cloud - {adv_id}",
                            "product_version": "N/A",
                            "oem_name": "Google",
                            "severity_level": standardize_severity(sev_text),
                            "vulnerability_description": f"Google Cloud Security Bulletin: {adv_id}",
                            "mitigation_strategy": f"See {adv_url}" if adv_url else "See Google advisory",
                            "published_date": standardize_date(date_text),
                            "cve_id": cve_id,
                            "url": adv_url or bulletin_url,
                        }
                        save_vulnerability_to_db(vulnerability)
                        vulnerabilities.append(vulnerability)
                    except Exception:
                        continue

            # If no table, try article/section cards
            if not vulnerabilities:
                for card in soup.select("article, section.security-bulletin, div.devsite-article-body h2"):
                    try:
                        title = card.get_text(strip=True)[:200]
                        link = card.find("a")
                        href = link.get("href", "") if link else ""
                        if href and not href.startswith("http"):
                            href = "https://cloud.google.com" + href
                        import re as _re
                        m = _re.search(r"CVE-\d{4}-\d+", str(card))
                        cve = m.group(0) if m else f"GCP-{hash(title) % 100000}"
                        vulnerability = {
                            "product_name": f"Google Cloud - {title[:80]}",
                            "product_version": "N/A",
                            "oem_name": "Google",
                            "severity_level": "Medium",
                            "vulnerability_description": title,
                            "mitigation_strategy": f"See {href}" if href else "See Google advisory",
                            "published_date": "",
                            "cve_id": cve,
                            "url": href or bulletin_url,
                        }
                        save_vulnerability_to_db(vulnerability)
                        vulnerabilities.append(vulnerability)
                    except Exception:
                        continue

            if vulnerabilities:
                logger.info(f"Found {len(vulnerabilities)} Google Cloud bulletins")
                return vulnerabilities
        except Exception as e:
            logger.warning(f"Google Cloud bulletins page failed: {e}")

        # --- Strategy 2: Android Security Bulletins (also Google OEM) ---
        android_url = "https://source.android.com/docs/security/bulletin"
        try:
            resp = session.get(android_url, headers=headers, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "html.parser")

            for link in soup.select("a[href*='security/bulletin/']"):
                href = link.get("href", "")
                text = link.get_text(strip=True)
                if not text or "bulletin" not in href.lower():
                    continue
                if not href.startswith("http"):
                    href = "https://source.android.com" + href

                import re as _re
                m = _re.search(r"(\d{4}-\d{2})", text + href)
                date_str = m.group(1) + "-01" if m else ""

                vulnerability = {
                    "product_name": f"Android - {text[:100]}",
                    "product_version": "Multiple",
                    "oem_name": "Google",
                    "severity_level": "High",
                    "vulnerability_description": f"Android Security Bulletin: {text}",
                    "mitigation_strategy": f"See {href}",
                    "published_date": standardize_date(date_str),
                    "cve_id": f"ANDROID-{hash(text) % 100000}",
                    "url": href,
                }
                save_vulnerability_to_db(vulnerability)
                vulnerabilities.append(vulnerability)

            if vulnerabilities:
                logger.info(f"Found {len(vulnerabilities)} Android/Google bulletins")
                return vulnerabilities
        except Exception as e:
            logger.warning(f"Android bulletins page failed: {e}")

        logger.warning("All Google scan strategies returned no results")
        return vulnerabilities

    except Exception as e:
        logger.error(f"Error in Google scanning: {str(e)}")
        return []


# ---- NEW OEM SCANNERS ----

def scan_microsoft():
    """Scan Microsoft Security Response Center (MSRC) for recent vulnerabilities."""
    try:
        logger.info("Scanning Microsoft Security Response Center...")
        vulnerabilities = []
        session = create_requests_session()
        headers = {
            "User-Agent": "VulnGuard/2.0 (Security Scanner)",
            "Accept": "application/json",
        }

        # --- Strategy 1: MSRC CVRF/CSAF API (public, no auth) ---
        # Get the list of recent security updates
        updates_url = "https://api.msrc.microsoft.com/cvrf/v3.0/updates"
        try:
            resp = session.get(updates_url, headers=headers, timeout=30)
            resp.raise_for_status()
            updates = resp.json().get("value", [])

            # Take most recent 3 months
            recent_ids = []
            for u in updates:
                doc_id = u.get("ID", "")
                # IDs look like "2024-Jan", "2024-Feb" etc.
                if doc_id:
                    recent_ids.append(doc_id)
            recent_ids = recent_ids[:3]  # last 3 entries (sorted newest-first by API)

            for doc_id in recent_ids:
                cvrf_url = f"https://api.msrc.microsoft.com/cvrf/v3.0/cvrf/{doc_id}"
                try:
                    r2 = session.get(cvrf_url, headers=headers, timeout=60)
                    if r2.status_code != 200:
                        continue
                    doc = r2.json()

                    vuln_list = doc.get("Vulnerability", [])
                    product_tree = doc.get("ProductTree", {})
                    # Build product ID -> name map
                    prod_map = {}
                    for branch in product_tree.get("Branch", []):
                        for item in branch.get("Items", []):
                            pid = item.get("ProductID", "")
                            pname = item.get("Value", "")
                            if pid and pname:
                                prod_map[pid] = pname

                    for vuln_entry in vuln_list:
                        cve_id = vuln_entry.get("CVE", "")
                        title = vuln_entry.get("Title", {}).get("Value", "")
                        notes = vuln_entry.get("Notes", [])
                        desc = ""
                        for note in notes:
                            if note.get("Type") == 1 or "description" in note.get("Title", "").lower():
                                desc = note.get("Value", "")
                                break
                        if not desc:
                            desc = title

                        # Severity from threats
                        severity = "Medium"
                        for threat in vuln_entry.get("Threats", []):
                            if threat.get("Type") == 3:  # Impact
                                desc_val = threat.get("Description", {}).get("Value", "")
                                if desc_val:
                                    severity = desc_val
                                    break

                        # Affected products
                        product_ids = set()
                        for status_entry in vuln_entry.get("ProductStatuses", []):
                            for pid in status_entry.get("ProductID", []):
                                product_ids.add(pid)
                        product_names = [prod_map.get(p, p) for p in list(product_ids)[:5]]
                        product_str = ", ".join(product_names) if product_names else "Microsoft Products"

                        # Remediation URL
                        rem_url = ""
                        for rem in vuln_entry.get("Remediations", []):
                            u = rem.get("URL", "")
                            if u:
                                rem_url = u
                                break

                        pub_date = vuln_entry.get("RevisionHistory", [{}])[0].get("Date", "") if vuln_entry.get("RevisionHistory") else ""

                        vulnerability = {
                            "product_name": product_str[:300],
                            "product_version": "Multiple",
                            "oem_name": "Microsoft",
                            "severity_level": standardize_severity(severity),
                            "vulnerability_description": (desc[:500] if desc else title),
                            "mitigation_strategy": f"See {rem_url}" if rem_url else f"See https://msrc.microsoft.com/update-guide/vulnerability/{cve_id}",
                            "published_date": standardize_date(pub_date.split("T")[0] if pub_date else ""),
                            "cve_id": cve_id if cve_id else f"MSRC-{doc_id}-{len(vulnerabilities)}",
                            "url": rem_url or f"https://msrc.microsoft.com/update-guide/vulnerability/{cve_id}",
                        }
                        save_vulnerability_to_db(vulnerability)
                        vulnerabilities.append(vulnerability)

                except Exception as e:
                    logger.warning(f"Failed to fetch MSRC CVRF document {doc_id}: {e}")
                    continue

            if vulnerabilities:
                logger.info(f"Found {len(vulnerabilities)} Microsoft vulnerabilities via MSRC API")
                return vulnerabilities

        except Exception as e:
            logger.warning(f"MSRC API failed: {e}")

        # --- Strategy 2: Scrape MSRC Update Guide page ---
        logger.info("Falling back to MSRC Update Guide scrape...")
        guide_url = "https://msrc.microsoft.com/update-guide/vulnerability"
        try:
            resp = session.get(guide_url, headers={**headers, "Accept": "text/html"}, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "html.parser")

            import re as _re
            for link in soup.find_all("a", href=_re.compile(r"CVE-\d{4}-\d+")):
                cve = link.get_text(strip=True)
                href = link.get("href", "")
                if not href.startswith("http"):
                    href = "https://msrc.microsoft.com" + href

                vulnerability = {
                    "product_name": "Microsoft Products",
                    "product_version": "Multiple",
                    "oem_name": "Microsoft",
                    "severity_level": "High",
                    "vulnerability_description": f"Microsoft Security Update: {cve}",
                    "mitigation_strategy": f"See {href}",
                    "published_date": "",
                    "cve_id": cve,
                    "url": href,
                }
                save_vulnerability_to_db(vulnerability)
                vulnerabilities.append(vulnerability)

            logger.info(f"Found {len(vulnerabilities)} Microsoft vulnerabilities via scrape")
        except Exception as e:
            logger.warning(f"MSRC scrape failed: {e}")

        return vulnerabilities

    except Exception as e:
        logger.error(f"Error in Microsoft scanning: {str(e)}")
        return []


def scan_fortinet():
    """Scan Fortinet/FortiGuard PSIRT advisories."""
    try:
        logger.info("Scanning Fortinet FortiGuard PSIRT advisories...")
        vulnerabilities = []
        session = create_requests_session()
        headers = {
            "User-Agent": "VulnGuard/2.0 (Security Scanner)",
            "Accept": "application/json, text/html",
        }

        # --- Strategy 1: FortiGuard PSIRT API ---
        api_url = "https://www.fortiguard.com/psirt"
        try:
            resp = session.get(api_url, headers={**headers, "Accept": "text/html"}, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "html.parser")

            # FortiGuard lists advisories in rows/cards
            import re as _re
            for item in soup.select("div.advisory-row, div.psirt-row, tr, div.result"):
                try:
                    link = item.find("a")
                    if not link:
                        continue
                    title = link.get_text(strip=True)
                    if not title or len(title) < 5:
                        continue
                    href = link.get("href", "")
                    if href and not href.startswith("http"):
                        href = "https://www.fortiguard.com" + href

                    # Try to find severity
                    sev_el = item.select_one("span.severity, td.severity, span.badge, div.severity")
                    severity = sev_el.get_text(strip=True) if sev_el else "High"

                    # Try to find date
                    date_el = item.select_one("span.date, td.date, div.date, time")
                    date_text = date_el.get_text(strip=True) if date_el else ""

                    # CVE from text
                    cve_match = _re.search(r"CVE-\d{4}-\d+", str(item))
                    cve_id = cve_match.group(0) if cve_match else ""

                    # Advisory ID (e.g. FG-IR-xx-xxx)
                    fg_match = _re.search(r"FG-IR-\d{2}-\d+", str(item))
                    fg_id = fg_match.group(0) if fg_match else ""

                    if not cve_id:
                        cve_id = fg_id if fg_id else f"FORTINET-{hash(title) % 100000}"

                    vulnerability = {
                        "product_name": title[:200],
                        "product_version": "Multiple",
                        "oem_name": "Fortinet",
                        "severity_level": standardize_severity(severity),
                        "vulnerability_description": title,
                        "mitigation_strategy": f"See {href}" if href else "See FortiGuard advisory",
                        "published_date": standardize_date(date_text) if date_text else "",
                        "cve_id": cve_id,
                        "url": href or api_url,
                    }
                    save_vulnerability_to_db(vulnerability)
                    vulnerabilities.append(vulnerability)
                except Exception:
                    continue

            if vulnerabilities:
                logger.info(f"Found {len(vulnerabilities)} Fortinet advisories")
                return vulnerabilities
        except Exception as e:
            logger.warning(f"FortiGuard PSIRT page failed: {e}")

        # --- Strategy 2: FortiGuard RSS ---
        rss_url = "https://filestore.fortinet.com/fortiguard/rss/ir.xml"
        try:
            resp = session.get(rss_url, headers=headers, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "xml")

            import re as _re
            for item in soup.find_all("item"):
                title = item.find("title").text.strip() if item.find("title") else ""
                link = item.find("link").text.strip() if item.find("link") else ""
                desc = item.find("description").text.strip() if item.find("description") else title
                pub_date = item.find("pubDate").text.strip() if item.find("pubDate") else ""

                cve_match = _re.search(r"CVE-\d{4}-\d+", desc + " " + title)
                cve_id = cve_match.group(0) if cve_match else f"FORTINET-{hash(title) % 100000}"

                vulnerability = {
                    "product_name": title[:200],
                    "product_version": "Multiple",
                    "oem_name": "Fortinet",
                    "severity_level": "High",
                    "vulnerability_description": desc[:500],
                    "mitigation_strategy": f"See {link}",
                    "published_date": standardize_date(pub_date),
                    "cve_id": cve_id,
                    "url": link,
                }
                save_vulnerability_to_db(vulnerability)
                vulnerabilities.append(vulnerability)

            logger.info(f"Found {len(vulnerabilities)} Fortinet advisories via RSS")
        except Exception as e:
            logger.warning(f"Fortinet RSS failed: {e}")

        return vulnerabilities

    except Exception as e:
        logger.error(f"Error in Fortinet scanning: {str(e)}")
        return []


def scan_palo_alto():
    """Scan Palo Alto Networks security advisories."""
    try:
        logger.info("Scanning Palo Alto Networks Security Advisories...")
        vulnerabilities = []
        session = create_requests_session()
        headers = {
            "User-Agent": "VulnGuard/2.0 (Security Scanner)",
            "Accept": "text/html",
        }

        advisories_url = "https://security.paloaltonetworks.com/"
        try:
            resp = session.get(advisories_url, headers=headers, timeout=30)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "html.parser")

            import re as _re

            # Try to find advisory list items / cards / rows
            selectors = [
                "div.advisory-item", "div.list-item", "article",
                "tr", "div.card", "li.advisory", "a[href*='CVE']",
                "a[href*='PAN-SA']",
            ]
            links_seen = set()

            for sel in selectors:
                for el in soup.select(sel):
                    try:
                        link = el if el.name == "a" else el.find("a")
                        if not link:
                            continue
                        href = link.get("href", "")
                        if href in links_seen:
                            continue
                        links_seen.add(href)

                        title = link.get_text(strip=True)
                        if not title or len(title) < 5:
                            continue
                        if href and not href.startswith("http"):
                            href = "https://security.paloaltonetworks.com" + href

                        sev_el = el.select_one("span.severity, span.badge, td:nth-child(2)")
                        severity = sev_el.get_text(strip=True) if sev_el else ""

                        date_el = el.select_one("span.date, time, td:nth-child(3)")
                        date_text = date_el.get_text(strip=True) if date_el else ""

                        cve_match = _re.search(r"CVE-\d{4}-\d+", str(el))
                        pan_match = _re.search(r"PAN-SA-\d{4}-\d+", str(el))
                        cve_id = cve_match.group(0) if cve_match else (pan_match.group(0) if pan_match else f"PAN-{hash(title) % 100000}")

                        vulnerability = {
                            "product_name": title[:200],
                            "product_version": "Multiple",
                            "oem_name": "Palo Alto",
                            "severity_level": standardize_severity(severity) if severity else "High",
                            "vulnerability_description": title,
                            "mitigation_strategy": f"See {href}" if href else "See Palo Alto advisory",
                            "published_date": standardize_date(date_text) if date_text else "",
                            "cve_id": cve_id,
                            "url": href or advisories_url,
                        }
                        save_vulnerability_to_db(vulnerability)
                        vulnerabilities.append(vulnerability)
                    except Exception:
                        continue
                if vulnerabilities:
                    break  # stop trying selectors once we find results

            if vulnerabilities:
                logger.info(f"Found {len(vulnerabilities)} Palo Alto advisories")
                return vulnerabilities
        except Exception as e:
            logger.warning(f"Palo Alto advisories page failed: {e}")

        # --- Strategy 2: Use NVD API filtered to Palo Alto Networks ---
        logger.info("Falling back to NVD API for Palo Alto vulnerabilities...")
        try:
            nvd_url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
            params = {
                "keywordSearch": "Palo Alto Networks PAN-OS",
                "resultsPerPage": 30,
            }
            resp = session.get(nvd_url, params=params, headers={"User-Agent": "VulnGuard/2.0"}, timeout=30)
            if resp.status_code == 200:
                data = resp.json()
                for item in data.get("vulnerabilities", []):
                    cve = item.get("cve", {})
                    cve_id = cve.get("id", "")
                    desc = ""
                    for d in cve.get("descriptions", []):
                        if d.get("lang") == "en":
                            desc = d.get("value", "")
                            break
                    severity = "Medium"
                    metrics = cve.get("metrics", {})
                    for key in ["cvssMetricV31", "cvssMetricV30", "cvssMetricV2"]:
                        metric_list = metrics.get(key, [])
                        if metric_list:
                            severity = metric_list[0].get("cvssData", {}).get("baseSeverity", "Medium")
                            break

                    vulnerability = {
                        "product_name": "Palo Alto PAN-OS",
                        "product_version": "Multiple",
                        "oem_name": "Palo Alto",
                        "severity_level": standardize_severity(severity),
                        "vulnerability_description": desc[:500],
                        "mitigation_strategy": f"See https://nvd.nist.gov/vuln/detail/{cve_id}",
                        "published_date": cve.get("published", "").split("T")[0],
                        "cve_id": cve_id,
                        "url": f"https://nvd.nist.gov/vuln/detail/{cve_id}",
                    }
                    save_vulnerability_to_db(vulnerability)
                    vulnerabilities.append(vulnerability)

                logger.info(f"Found {len(vulnerabilities)} Palo Alto vulns via NVD")
        except Exception as e:
            logger.warning(f"NVD fallback for Palo Alto failed: {e}")

        return vulnerabilities

    except Exception as e:
        logger.error(f"Error in Palo Alto scanning: {str(e)}")
        return []


def scan_adobe():
    """Scan Adobe PSIRT security bulletins."""
    try:
        logger.info("Scanning Adobe Security Bulletins...")
        vulnerabilities = []
        import re as _re

        # Use a plain session (no retry adapter) with a short timeout so we
        # don't burn minutes on hosts that are blocking/timing-out.
        fast_session = requests.Session()
        html_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                          "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml",
            "Accept-Language": "en-US,en;q=0.9",
        }

        # --- Strategy 1: Adobe Security landing page (lighter than per-product pages) ---
        landing_url = "https://helpx.adobe.com/security.html"
        product_pages = [
            (landing_url, "Adobe"),
            ("https://helpx.adobe.com/security/products/acrobat.html", "Adobe Acrobat/Reader"),
            ("https://helpx.adobe.com/security/products/coldfusion.html", "Adobe ColdFusion"),
        ]

        for page_url, product_label in product_pages:
            try:
                resp = fast_session.get(page_url, headers=html_headers, timeout=12)
                if resp.status_code != 200:
                    continue
                soup = BeautifulSoup(resp.content, "html.parser")

                # Parse tables
                for table in soup.find_all("table"):
                    rows = table.find_all("tr")[1:]
                    for row in rows:
                        cells = row.find_all("td")
                        if len(cells) < 2:
                            continue
                        try:
                            link = row.find("a")
                            title_text = link.get_text(strip=True) if link else cells[0].get_text(strip=True)
                            href = link.get("href", "") if link else ""
                            if href and not href.startswith("http"):
                                href = "https://helpx.adobe.com" + href

                            date_text = ""
                            sev_text = ""
                            for cell in cells:
                                ct = cell.get_text(strip=True)
                                if _re.match(r"\w+ \d{1,2},? \d{4}", ct) or _re.match(r"\d{4}-\d{2}", ct):
                                    date_text = ct
                                elif ct.lower() in ("critical", "important", "moderate", "low", "high", "medium"):
                                    sev_text = ct

                            cve_match = _re.search(r"CVE-\d{4}-\d+", str(row))
                            apsb_match = _re.search(r"APSB\d{2}-\d+", str(row))
                            cve_id = cve_match.group(0) if cve_match else (
                                apsb_match.group(0) if apsb_match else f"ADOBE-{hash(title_text) % 100000}")

                            vulnerability = {
                                "product_name": f"{product_label} - {title_text[:120]}",
                                "product_version": "Multiple",
                                "oem_name": "Adobe",
                                "severity_level": standardize_severity(sev_text) if sev_text else "High",
                                "vulnerability_description": f"Adobe Security Bulletin: {title_text[:300]}",
                                "mitigation_strategy": f"See {href}" if href else "See Adobe security bulletin",
                                "published_date": standardize_date(date_text) if date_text else "",
                                "cve_id": cve_id,
                                "url": href or page_url,
                            }
                            save_vulnerability_to_db(vulnerability)
                            vulnerabilities.append(vulnerability)
                        except Exception:
                            continue

                # APSB-style links
                for link in soup.select("a[href*='apsb'], a[href*='security']"):
                    href = link.get("href", "")
                    text = link.get_text(strip=True)
                    if not text or len(text) < 5:
                        continue
                    apsb_match = _re.search(r"APSB\d{2}-\d+", href + " " + text)
                    if not apsb_match:
                        continue
                    if href and not href.startswith("http"):
                        href = "https://helpx.adobe.com" + href
                    cve_id = apsb_match.group(0)
                    if any(v["cve_id"] == cve_id for v in vulnerabilities):
                        continue
                    vulnerability = {
                        "product_name": f"{product_label} - {text[:120]}",
                        "product_version": "Multiple",
                        "oem_name": "Adobe",
                        "severity_level": "High",
                        "vulnerability_description": f"Adobe Security Bulletin: {text[:300]}",
                        "mitigation_strategy": f"See {href}",
                        "published_date": "",
                        "cve_id": cve_id,
                        "url": href,
                    }
                    save_vulnerability_to_db(vulnerability)
                    vulnerabilities.append(vulnerability)

            except (requests.ReadTimeout, requests.ConnectionError) as e:
                logger.warning(f"Adobe page {page_url} timed out / unreachable — skipping ({e})")
                continue
            except Exception as e:
                logger.warning(f"Adobe page {page_url} failed: {e}")
                continue

        if vulnerabilities:
            logger.info(f"Found {len(vulnerabilities)} Adobe security bulletins")
            return vulnerabilities

        # --- Strategy 2: NVD API fallback (always reachable) ---
        logger.info("Adobe pages unreachable — falling back to NVD API for Adobe vulnerabilities...")
        nvd_session = create_requests_session()
        try:
            nvd_url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
            params = {"keywordSearch": "Adobe Acrobat Reader", "resultsPerPage": 30}
            resp = nvd_session.get(nvd_url, params=params,
                                   headers={"User-Agent": "VulnGuard/2.0"}, timeout=30)
            if resp.status_code == 200:
                data = resp.json()
                for item in data.get("vulnerabilities", []):
                    cve = item.get("cve", {})
                    cve_id = cve.get("id", "")
                    desc = ""
                    for d in cve.get("descriptions", []):
                        if d.get("lang") == "en":
                            desc = d.get("value", "")
                            break
                    severity = "Medium"
                    metrics = cve.get("metrics", {})
                    for key in ["cvssMetricV31", "cvssMetricV30", "cvssMetricV2"]:
                        metric_list = metrics.get(key, [])
                        if metric_list:
                            severity = metric_list[0].get("cvssData", {}).get("baseSeverity", "Medium")
                            break
                    vulnerability = {
                        "product_name": "Adobe Products",
                        "product_version": "Multiple",
                        "oem_name": "Adobe",
                        "severity_level": standardize_severity(severity),
                        "vulnerability_description": desc[:500],
                        "mitigation_strategy": f"See https://nvd.nist.gov/vuln/detail/{cve_id}",
                        "published_date": cve.get("published", "").split("T")[0],
                        "cve_id": cve_id,
                        "url": f"https://nvd.nist.gov/vuln/detail/{cve_id}",
                    }
                    save_vulnerability_to_db(vulnerability)
                    vulnerabilities.append(vulnerability)
                logger.info(f"Found {len(vulnerabilities)} Adobe vulns via NVD")
        except Exception as e:
            logger.warning(f"NVD fallback for Adobe failed: {e}")

        return vulnerabilities

    except Exception as e:
        logger.error(f"Error in Adobe scanning: {str(e)}")
        return []


def openvas_supported() -> bool:
    """Return True when OpenVAS dependencies are available."""

    return OPENVAS_DEPENDENCIES_AVAILABLE or docker_backend_available()


def scan_openvas_targets(
    targets: List[str],
    scan_name: Optional[str] = None,
    custom_config: Optional[Any] = None,
    severity_filter: Optional[List[str]] = None,
) -> Tuple[List[Dict[str, str]], Any]:
    """Run an OpenVAS scan, persist the results, and return new findings."""

    config = custom_config or load_openvas_config()
    scan_result = run_openvas_scan(targets=targets, scan_name=scan_name, config=config)

    allowed = {level.capitalize() for level in severity_filter} if severity_filter else None
    candidate_vulns = (
        [v for v in scan_result.vulnerabilities if v.get("severity_level") in allowed]
        if allowed
        else list(scan_result.vulnerabilities)
    )

    persisted: List[Dict[str, str]] = []
    for vuln in candidate_vulns:
        if save_vulnerability_to_db(vuln):
            persisted.append(vuln)

    logger.info(
        "OpenVAS scan '%s' produced %s findings (%s persisted, %s duplicates)",
        scan_result.scan_name,
        len(candidate_vulns),
        len(persisted),
        len(candidate_vulns) - len(persisted),
    )

    return persisted, scan_result

# Initialize database if needed
if __name__ == "__main__":
    setup_database()
    
    # For testing specific scanners
    import sys
    if len(sys.argv) > 1:
        source = sys.argv[1]
        if source in OEM_SOURCES:
            vulnerabilities = scan_source(source)
            print(f"Found {len(vulnerabilities)} vulnerabilities from {source}")
            for vuln in vulnerabilities[:5]:  # Show first 5 for testing
                print(f"- {vuln.get('cve_id', 'N/A')}: {vuln.get('product_name', 'N/A')} ({vuln.get('severity_level', 'N/A')})")
        else:
            print(f"Source '{source}' not found in configured sources")
            print(f"Available sources: {', '.join(OEM_SOURCES.keys())}")
    else:
        # Default behavior: scan NVD
        nvd_vulnerabilities = scan_nvd() 